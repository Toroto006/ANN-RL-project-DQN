{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epidemic mitigation project\n",
    "\n",
    "For question 3 we reuse the tutorial environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the relevant packages\n",
    "If the environment is correctly setup you should be able to sucessfully import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.26.2 in /opt/conda/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from gym==0.26.2) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.9/site-packages (from gym==0.26.2) (4.10.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.9/site-packages (from gym==0.26.2) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from gym==0.26.2) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym==0.26.2) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 69\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code to make the picture a bit nicer for the report\n",
    "SAVE_REPORT = False\n",
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    # Use 10pt font in plots, to match 10pt font in document\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"font.size\": 10,\n",
    "    # Make the legend/label fonts a little smaller\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8\n",
    "}\n",
    "# does need cm-super, dvipng and latex to be installed\n",
    "if SAVE_REPORT:\n",
    "        plt.rcParams.update(tex_fonts)\n",
    "\n",
    "def set_size(width, fraction=1):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "    from: https://jwalton.info/Embed-Publication-Matplotlib-Latex/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "    return fig_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DQN agent, with binary action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2070 SUPER\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "SCALE = 100\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "        \n",
    "    return action\n",
    "\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    obvSpace = torch.Tensor(np.stack((infected, dead))).to(device).unsqueeze(0)\n",
    "    return obvSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number cities: 9 and env_step_length: 7\n",
      "sampled action : 0\n",
      "Sampled observation (shape (2, 9, 7)) first dimension is infected & dead\n"
     ]
    }
   ],
   "source": [
    "action_space        =   spaces.Discrete(2)\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "print(f\"Number cities: {dyn.n_cities} and env_step_length: {dyn.env_step_length}\")\n",
    "print(f\"sampled action : {action_space.sample()}\")\n",
    "example_obs = observation_space.sample()\n",
    "print(f\"Sampled observation (shape {example_obs.shape}) first dimension is infected & dead\")\n",
    "#plt.matshow(observation_space.sample()[0,:,:])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            action_preprocessor=action_preprocessor\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a) implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(n_observations, 64)\n",
    "        self.layer1 = nn.Linear(64, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # TODO maybe 1/4 scale?\n",
    "        x = x**0.25\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters given by the assignment\n",
    "# TODO maybe try some hyperparameter tuning after we know the DQN works\n",
    "LR = 5*10**-3\n",
    "DF = 0.9\n",
    "MS = 20_000\n",
    "BS = 2048\n",
    "TAU = 0.5\n",
    "\n",
    "update_every = 5 # episodes\n",
    "episodes = 500\n",
    "average_over = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function used later\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent(Agent):\n",
    "    def __init__(self,  env:Env, eps=0.7):\n",
    "        self.env = env\n",
    "        self.n_observations = spaces.utils.flatdim(env.observation_space)\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.dqnQtheta = DQN(self.n_observations, self.n_actions).to(device)\n",
    "        self.dqnQhat = DQN(self.n_observations, self.n_actions).to(device)\n",
    "        self.dqnQhat.load_state_dict(self.dqnQtheta.state_dict())\n",
    "        self.memory = ReplayMemory(MS)\n",
    "        self.optimizer = optim.AdamW(self.dqnQtheta.parameters(), lr=LR, amsgrad=True)\n",
    "        self.epsilon = eps\n",
    "\n",
    "    def load_model(self, savepath):\n",
    "        loaded_state_dict = torch.load(savepath)    \n",
    "        self.dqnQtheta.load_state_dict(loaded_state_dict)\n",
    "        self.dqnQhat.load_state_dict(loaded_state_dict)\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        torch.save(self.dqnQtheta.state_dict(), savepath)\n",
    "        \n",
    "    # Following alg. 1 in https://arxiv.org/pdf/1312.5602.pdf\n",
    "    def optimize_model(self,):\n",
    "        # Heavily based on https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "        if len(self.memory) < BS:\n",
    "            return\n",
    "        transitions = self.memory.sample(BS)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "        #print(f\"non_final_mask: {non_final_mask.shape}\") #torch.Size([BS])\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "        # Compute  Q_sigma(sj, aj) for all states.\n",
    "        state_action_values = self.dqnQtheta(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute rj + DF * max_a' Q_hat(sj+1, a') for all non-final states,\n",
    "        # for all final states values are 0\n",
    "        next_state_values = torch.zeros(BS, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.dqnQhat(non_final_next_states).max(1)[0]\n",
    "        next_state_values = next_state_values.unsqueeze(1)\n",
    "        expected_state_action_values = reward_batch + (DF * next_state_values)\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model, where we update Q_0 & Q_hat at the same time\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.dqnQtheta.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        torch.manual_seed(SEED)\n",
    "        # torch.use_deterministic_algorithms(True) # TODO, need to fix that this works\n",
    "        pass\n",
    "    \n",
    "    def act(self, obs):\n",
    "        # eps greedy choosing\n",
    "        if random.random() < 1 - self.epsilon:\n",
    "            # Exploitation\n",
    "            with torch.no_grad():\n",
    "                # The unmitigated agent always takes the same action, i.e. do nothing\n",
    "                containment = self.dqnQtheta(obs.unsqueeze(0)) # flatten not necessary bc of nn.Flatten\n",
    "                # max(1) returns lagest column vector per row, [1] of that the index of it \n",
    "                return containment.max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            # Exploration\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the agent over one episode\n",
    "def run_episode(agent, env, seed=None, soft_update=False):\n",
    "    finished = False\n",
    "    if seed is None:\n",
    "        obs, _ = env.reset()\n",
    "    else:\n",
    "        obs, _ = env.reset(seed)\n",
    "    rewards = []\n",
    "    while not finished:\n",
    "        # Each action is done per week\n",
    "        action = agent.act(obs)\n",
    "        # Meaning also the log info is per week\n",
    "        new_obs, R, finished, _ = env.step(action)\n",
    "        # Log this step\n",
    "        #deaths.append(info.total.dead)\n",
    "        rewards.append(R)\n",
    "        #log.append(info) # save the information dict for logging\n",
    "        if seed is None:\n",
    "            # If terminal => no next state exits\n",
    "            next_obs = None if finished else new_obs\n",
    "            # Save to replay buffer\n",
    "            agent.memory.push(obs, action, next_obs, R)\n",
    "            # Move to the next state\n",
    "            obs = next_obs\n",
    "            # Optimize the model\n",
    "            agent.optimize_model()\n",
    "            # Soft update every step of the way\n",
    "            if soft_update:\n",
    "                Q_theta = agent.dqnQtheta.state_dict()\n",
    "                Q_hat = agent.dqnQhat.state_dict()\n",
    "                for key in Q_hat:\n",
    "                    Q_hat[key] = Q_theta[key]*TAU + Q_hat[key]*(1-TAU)\n",
    "                agent.dqnQhat.load_state_dict(Q_hat)\n",
    "            \n",
    "    return np.sum([r.numpy()[0] for r in rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_reward = -np.inf\n",
    "\n",
    "def evaluate_agent(agent, env, seed, eval_episodes=20, save_best=True):\n",
    "    last_eps = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    eval_rewards = []\n",
    "    for i in range(eval_episodes):\n",
    "        eval_rewards.append(run_episode(agent, env, seed+i))\n",
    "    mean_eval_reward = np.mean(eval_rewards)\n",
    "    agent.epsilon = last_eps\n",
    "    # check if this is the best reward and if so save the model\n",
    "    global best_eval_reward\n",
    "    if save_best and mean_eval_reward > best_eval_reward:\n",
    "        best_eval_reward = mean_eval_reward\n",
    "        agent.save_model(savepath=\"Q3_best_model.mdl\")\n",
    "        print(f\"Best model updated with mean eval reward: {mean_eval_reward:.2f}\")\n",
    "    return mean_eval_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model updated with mean eval reward: 23.32\n",
      "Episode  51/500 (10.00%) - eval reward: 23.32\n",
      "Episode 101/500 (20.00%) - eval reward: -26.98\n",
      "Episode 151/500 (30.00%) - eval reward: -38.07\n",
      "Episode 171/500 (34.00%)\r"
     ]
    }
   ],
   "source": [
    "def run_simulation(agent,env,seed):\n",
    "    \"\"\" Run the simulation \"\"\"\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    for i in range(episodes):\n",
    "        print(f\"Episode {i+1:3}/{episodes} ({i*100/episodes:.2f}%)\", end=\"\\r\")\n",
    "        reward = run_episode(agent, env)\n",
    "        training_trace.append(reward)\n",
    "        if i % update_every == 0:\n",
    "            # Update the target network fully, copying all weights and biases\n",
    "            agent.dqnQhat.load_state_dict(agent.dqnQtheta.state_dict())\n",
    "        if (i % 50 == 0 or i == episodes-1) and i != 0:\n",
    "            # run eval 20 episodes average with epsilon of 0\n",
    "            mean_eval_reward = evaluate_agent(agent, env, seed, eval_episodes=20)\n",
    "            print(f\"Episode {i+1:3}/{episodes} ({i*100/episodes:.2f}%) - eval reward: {mean_eval_reward:.2f}\")\n",
    "            eval_trace.append(mean_eval_reward)\n",
    "\n",
    "    return training_trace, eval_trace\n",
    "\n",
    "# Eval trace averaged over n runs, training_trace can be used together\n",
    "training_trace, eval_trace = list(zip(*[run_simulation(DQNagent(env),env,SEED) for _ in range(average_over)]))\n",
    "training_trace = flatten(training_trace)\n",
    "eval_trace = np.mean(eval_trace, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trace(name, combined):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(combined, f)\n",
    "\n",
    "def load_trace(name='Q3a-trace.pickle'):\n",
    "    with open(name, 'rb') as f:\n",
    "        combined = pickle.load(f)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_a = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": flatten([[i]*average_over for i in range(int(len(training_trace)/average_over))]),\n",
    "        \"Reward\": training_trace,\n",
    "        \"Trace\": [\"Training\" for _ in range(len(training_trace))],\n",
    "        \"Exploration\": [\"Fixed\" for _ in range(len(training_trace))]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": [50 * i for i in range(len(eval_trace))],\n",
    "        \"Reward\": eval_trace,\n",
    "        \"Trace\": [\"Evaluation\" for _ in range(len(eval_trace))],\n",
    "        \"Exploration\": [\"Fixed\" for _ in range(len(eval_trace))]\n",
    "    })\n",
    "])\n",
    "save_trace('Q3a-trace.pickle', combined_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_a = load_trace(name='Q3a-trace.pickle')\n",
    "sns.scatterplot(data=combined_a, x=\"Episode\", y=\"Reward\", hue=\"Trace\")\n",
    "plt.title(\"Training and evaluation trace of DQN agent\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Does your agent learn a meaningful policy?\n",
    "It does not seem that this agent learns a meaningful policy, because the reward does not seem to increase by much over the training time.\n",
    "This might not be completely the fault of this agent though, because this agent is not allowed to touch any other action than confinment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record three example episodes with best policy and plot one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNagent(env)\n",
    "agent.load_model(\"Q3_best_model.mdl\")\n",
    "\n",
    "def run_episode_simulation(agent,env,seed):\n",
    "    \"\"\" Run the simulation \"\"\"\n",
    "    log = []\n",
    "    finished = False\n",
    "    obs, info = env.reset(seed)\n",
    "    deaths = []\n",
    "    rewards = []\n",
    "    agent.reset()\n",
    "    agent.epsilon = 0\n",
    "    while not finished:\n",
    "        # Each action is done per week\n",
    "        action = agent.act(obs)\n",
    "        # Meaning also the log info is per week\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        deaths.append(info.total.dead)\n",
    "        rewards.append(R)\n",
    "        log.append(info) # save the information dict for logging\n",
    "\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    deaths = np.array(deaths)\n",
    "    rewards = np.array([r.numpy()[0] for r in rewards])\n",
    "    total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "    return deaths, rewards, total, cities, actions\n",
    "\n",
    "three_example_episodes = [run_episode_simulation(agent,env,SEED+i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the example episodes and plot the results\n",
    "deaths, rewards, total, cities, actions = three_example_episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single plot\n",
    "size = set_size(700) if SAVE_REPORT else (14, 10)\n",
    "fig = plt.figure(figsize=size)\n",
    "ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "[ax_leftstate.plot(y) for y in total.values()]\n",
    "ax_leftstate.legend([t.title() for t in total.keys()])\n",
    "ax_leftstate.set_title('Full state information of the simulation')\n",
    "ax_leftstate.set_ylabel('Number of people in each state')\n",
    "\n",
    "[ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "ax_leftobs.legend(['Infected','Dead'])\n",
    "ax_leftobs.set_title('Observable state information of the simulation')\n",
    "ax_leftobs.set_ylabel('Number of people in each state')\n",
    "\n",
    "ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "ax_leftactions.set_title('Actions taken by the agent')\n",
    "ax_leftactions.set_yticks([0,1,2,3])\n",
    "ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "ax_leftactions.set_xlabel('Weeks in episode')\n",
    "\n",
    "[ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "[ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "[ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "[ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "ax_right['Zürich'].set_xlabel('Weeks in episode')\n",
    "ax_right['Zürich'].xaxis.set_major_locator(ticker.MultipleLocator(2.000))\n",
    "ax_right['Lausanne'].set_title('Observable state per city')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.tight_layout()\n",
    "if SAVE_REPORT:\n",
    "    plt.savefig('figures/created/Q3-a-complete.png', dpi=300)\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Interpret this policy.\n",
    "From looking at a few full episodes using this policy, it seems to me that this best policy for now just tries to balance keeping everyone under confinement and the cost of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b) decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent,env,seed, eps_0=0.7, eps_min=0.2):\n",
    "    \"\"\" Run the simulation \"\"\"\n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    for i in range(episodes):\n",
    "        print(f\"Episode {i+1:3}/{episodes} ({i*100/episodes:.2f}%)\", end=\"\\r\")\n",
    "        agent.epsilon = max(eps_0*(episodes-i)/episodes, eps_min)\n",
    "        reward = run_episode(agent, env)\n",
    "        training_trace.append(reward)\n",
    "        if i % update_every == 0:\n",
    "            # Update the target network fully, copying all weights and biases\n",
    "            agent.dqnQhat.load_state_dict(agent.dqnQtheta.state_dict())\n",
    "        if (i % 50 == 0 or i == episodes-1) and i != 0:\n",
    "            # run eval 20 episodes average with epsilon of 0\n",
    "            mean_eval_reward = evaluate_agent(agent, env, seed, eval_episodes=20)\n",
    "            print(f\"Episode {i+1:3}/{episodes} ({i*100/episodes:.2f}%) - eval reward: {mean_eval_reward:.2f}\")\n",
    "            eval_trace.append(mean_eval_reward)\n",
    "    \n",
    "    return training_trace, eval_trace\n",
    "\n",
    "# Eval trace averaged over n runs, training_trace can be used together\n",
    "training_trace_b, eval_trace_b = list(zip(*[run_simulation(DQNagent(env),env,SEED) for _ in range(average_over)]))\n",
    "training_trace_b = flatten(training_trace_b)\n",
    "eval_trace_b = np.mean(eval_trace_b, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_b = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": flatten([[i]*average_over for i in range(int(len(training_trace_b)/average_over))]),\n",
    "        \"Reward\": training_trace_b,\n",
    "        \"Trace\": [\"Training\" for _ in range(len(training_trace_b))],\n",
    "        \"Exploration\": [\"Decreasing\" for _ in range(len(training_trace_b))]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": [50 * i for i in range(len(eval_trace_b))],\n",
    "        \"Reward\": eval_trace_b,\n",
    "        \"Trace\": [\"Evaluation\" for _ in range(len(eval_trace_b))],\n",
    "        \"Exploration\": [\"Decreasing\" for _ in range(len(eval_trace_b))]\n",
    "    })\n",
    "])\n",
    "save_trace('Q3b-trace.pickle', combined_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_b = load_trace(name='Q3b-trace.pickle')\n",
    "sns.scatterplot(data=combined_b, x=\"Episode\", y=\"Reward\", hue=\"Trace\")\n",
    "plt.title(\"Training and eval. trace of DQN agent with decreasing expl.\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them together\n",
    "combined_a = load_trace(name='Q3a-trace.pickle')\n",
    "combined_b = load_trace(name='Q3b-trace.pickle')\n",
    "combined = pd.concat([combined_a, combined_b])\n",
    "sns.scatterplot(data=combined, x=\"Episode\", y=\"Reward\", hue=\"Trace\", style=\"Exploration\")\n",
    "plt.title(\"Training & eval. trace of DQN agent with diff. exploration.\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Compare and discuss the results between the two different exploration policies. Which policy gets the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple simulations\n",
    "Running a suimulation of 50 enviroments with the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(agent,env,n):\n",
    "    info = []\n",
    "    for i in range(n):\n",
    "        # Run the agent with seed i, returns deaths, rewards, total, cities, actions\n",
    "        info.append(run_episode_simulation(agent,env,i))\n",
    "    return info\n",
    "info = simulation(agent,env,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process simulations\n",
    "conf_days = [np.sum(c[4][\"confinement\"])*7 for c in info]\n",
    "rewards = [np.mean(c[1]) for c in info]\n",
    "deaths = [d[0][-1] for d in info] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" As it is deterministic we really don't need to get anything else than deaths and rewards \"\"\"\n",
    "\"\"\" Plot example \"\"\"\n",
    "fig, ax = plt.subplots(1,3,figsize=(12,4))\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n",
    "hist_avg(ax[0], deaths,'deaths')\n",
    "hist_avg(ax[1], rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], conf_days,'confined days')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print example \"\"\"\n",
    "print(f'Average death number: {np.mean(deaths)}')\n",
    "print(f'Average cumulative reward: {np.mean(rewards)}')\n",
    "print(f'Average number of confined days: {np.mean(conf_days)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Did the reinforcement learning policy outperform Pr. Russo's policy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ca8a34e536ae65e073071989487f43d99ac250feafd80421457c1fca6e7506f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
