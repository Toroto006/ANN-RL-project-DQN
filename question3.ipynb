{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epidemic mitigation project\n",
    "\n",
    "For question 3 we reuse the tutorial environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the relevant packages\n",
    "If the environment is correctly setup you should be able to sucessfully import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 69\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code to make the picture a bit nicer for the report\n",
    "SAVE_REPORT = False\n",
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    # Use 10pt font in plots, to match 10pt font in document\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"font.size\": 10,\n",
    "    # Make the legend/label fonts a little smaller\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8\n",
    "}\n",
    "# does need cm-super, dvipng and latex to be installed\n",
    "if SAVE_REPORT:\n",
    "        plt.rcParams.update(tex_fonts)\n",
    "\n",
    "def set_size(width, fraction=1):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "    from: https://jwalton.info/Embed-Publication-Matplotlib-Latex/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "    return fig_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DQN agent, with binary action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "#ACTION_ISOLATE = 2\n",
    "#ACTION_HOSPITAL = 3\n",
    "#ACTION_VACCINATE = 4\n",
    "SCALE = 100\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "        \n",
    "    return action\n",
    "\n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].dead)/obs.pop[c] for c in dyn.cities])\n",
    "    obvSpace = torch.Tensor(np.stack((infected, dead))).unsqueeze(0)\n",
    "    return obvSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number cities: 9 and env_step_length: 7\n",
      "sampled action : 0\n",
      "Sampled observation (shape (2, 9, 7)) first dimension is infected & dead\n"
     ]
    }
   ],
   "source": [
    "action_space        =   spaces.Discrete(2)\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "print(f\"Number cities: {dyn.n_cities} and env_step_length: {dyn.env_step_length}\")\n",
    "print(f\"sampled action : {action_space.sample()}\")\n",
    "example_obs = observation_space.sample()\n",
    "print(f\"Sampled observation (shape {example_obs.shape}) first dimension is infected & dead\")\n",
    "#plt.matshow(observation_space.sample()[0,:,:])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            action_preprocessor=action_preprocessor\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we show how to subclass the agent class. (Here to create a fully random dummy agent with 5 actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(n_observations, 64)\n",
    "        self.layer1 = nn.Linear(64, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # TODO maybe 1/4 scale?\n",
    "        x = x**0.25\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5*10**-3\n",
    "DF = 0.9\n",
    "MS = 20_000\n",
    "#BS = 2048 # Too big for my machine\n",
    "BS = 256\n",
    "\n",
    "update_every = 5 # episodes\n",
    "episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent(Agent):\n",
    "    def __init__(self,  env:Env, eps=0.7):\n",
    "        self.env = env\n",
    "        self.n_observations = spaces.utils.flatdim(env.observation_space)\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.dqnQtheta = DQN(self.n_observations, self.n_actions)#.to(device)\n",
    "        self.dqnQhat = DQN(self.n_observations, self.n_actions)#.to(device)\n",
    "        self.dqnQhat.load_state_dict(self.dqnQtheta.state_dict())\n",
    "        self.memory = ReplayMemory(MS)\n",
    "        self.optimizer = optim.AdamW(self.dqnQtheta.parameters(), lr=LR, amsgrad=True)\n",
    "        self.epsilon = eps\n",
    "\n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        loaded_state_dict = torch.load(savepath)    \n",
    "        self.dqnQtheta.load_state_dict(loaded_state_dict)\n",
    "        self.dqnQhat.load_state_dict(loaded_state_dict)\n",
    "        #self.memory.reset()\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        torch.save(self.dqnQtheta.state_dict(), savepath)\n",
    "        \n",
    "    # Following alg. 1 in https://arxiv.org/pdf/1312.5602.pdf\n",
    "    def optimize_model(self,):\n",
    "        if len(self.memory) < BS:\n",
    "            return\n",
    "        transitions = self.memory.sample(BS)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.bool)\n",
    "        #print(f\"non_final_mask: {non_final_mask.shape}\") #torch.Size([BS])\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute  Q_sigma(sj, aj) for all states.\n",
    "        state_action_values = self.dqnQtheta(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute rj + DF * max_a' Q_hat(sj+1, a') for all non-final states,\n",
    "        # for all final states values are 0\n",
    "        next_state_values = torch.zeros(BS)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.dqnQhat(non_final_next_states).max(1)[0]\n",
    "        next_state_values = next_state_values.unsqueeze(1)\n",
    "        expected_state_action_values = reward_batch + (DF * next_state_values)\n",
    "        #expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model, where we update Q_0 & Q_hat at the same time\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.dqnQtheta.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        pass\n",
    "    \n",
    "    def act(self, obs):\n",
    "        # eps greedy choosing\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploitation\n",
    "            with torch.no_grad():\n",
    "                # The unmitigated agent always takes the same action, i.e. do nothing\n",
    "                containment = self.dqnQtheta(obs.unsqueeze(0)) # flatten not necessary bc of nn.Flatten\n",
    "                # max(1) returns lagest column vector per row, [1] of that the index of it \n",
    "                return containment.max(1)[1].view(1, 1)\n",
    "                #return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "        else:\n",
    "            # Exploration\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500 (0.00%) - eval reward: -140.56271362304688\n",
      "Episode 51/500 (10.00%) - eval reward: -159.33084106445312\n",
      "Episode 99/500 (19.60%)\r"
     ]
    }
   ],
   "source": [
    "agent = DQNagent(env)\n",
    "best_eval_reward = -np.inf\n",
    "\n",
    "def run_simulation(agent,env,seed):\n",
    "    \"\"\" Run the simulation \"\"\"\n",
    "    log = []\n",
    "    def run_episode(seed=None):\n",
    "        finished = False\n",
    "        # TODO do we want to seed the env on reset?\n",
    "        # Don't we then learn a single perfect strategy for that single evolution?\n",
    "        if seed is None:\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            obs, _ = env.reset(seed)\n",
    "        rewards = []\n",
    "        while not finished:\n",
    "            # Each action is done per week\n",
    "            action = agent.act(obs)\n",
    "            # Meaning also the log info is per week\n",
    "            new_obs, R, finished, _ = env.step(action)\n",
    "            # Log this step\n",
    "            #deaths.append(info.total.dead)\n",
    "            rewards.append(R)\n",
    "            #log.append(info) # save the information dict for logging\n",
    "\n",
    "            # If terminal => no next state exits\n",
    "            next_obs = None if finished else new_obs\n",
    "            # Save to replay buffer\n",
    "            agent.memory.push(obs, action, next_obs, R)\n",
    "            # Move to the next state\n",
    "            obs = next_obs\n",
    "            # Optimize the model\n",
    "            agent.optimize_model()\n",
    "                \n",
    "        return np.sum([r.numpy()[0] for r in rewards])\n",
    "    \n",
    "    training_trace = []\n",
    "    eval_trace = []\n",
    "    for i in range(episodes):\n",
    "        print(f\"Episode {i+1}/{episodes} ({i*100/episodes:.2f}%)\", end=\"\\r\")\n",
    "        reward = run_episode()\n",
    "        training_trace.append(reward)\n",
    "        if i % update_every == 0:\n",
    "            # Update the target network fully, copying all weights and biases\n",
    "            agent.dqnQhat.load_state_dict(agent.dqnQtheta.state_dict())\n",
    "        if i % 50 == 0 or i == episodes-1:\n",
    "            # run eval 20 episodes average with epsilon of 0\n",
    "            agent.epsilon = 0\n",
    "            eval_rewards = []\n",
    "            for _ in range(20):\n",
    "                eval_rewards.append(run_episode(seed))\n",
    "            mean_eval_reward = np.mean(eval_rewards)\n",
    "            eval_trace.append(mean_eval_reward)\n",
    "            agent.epsilon = 0.7\n",
    "            print(f\"Episode {i+1}/{episodes} ({i*100/episodes:.2f}%) - eval reward: {mean_eval_reward}\")\n",
    "            # check if this is the best reward and if so save the model\n",
    "            global best_eval_reward\n",
    "            if mean_eval_reward > best_eval_reward:\n",
    "                best_eval_reward = mean_eval_reward\n",
    "                agent.save_model(savepath=\"Q3a_best_model.mdl\")\n",
    "\n",
    "    return training_trace, eval_trace\n",
    "\n",
    "# Eval trace averaged over 3 runs, training_trace can be used together\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "training_trace, eval_trace = list(zip(*[run_simulation(agent,env,SEED) for _ in range(3)]))\n",
    "training_trace = flatten(training_trace)\n",
    "eval_trace = np.mean(eval_trace, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": flatten([[i]*3 for i in range(len(training_trace))]),\n",
    "        \"Reward\": training_trace,\n",
    "        \"Type\": \"Training trace\"\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Episode\": [50 * i for i in range(len(eval_trace))],\n",
    "        \"Reward\": eval_trace,\n",
    "        \"Type\": \"Evaluation trace\"\n",
    "    })\n",
    "])\n",
    "sns.scatterplot(combined, x=\"Episode\", y=\"Reward\", hue=\"Type\")\n",
    "plt.title(\"Training and evaluation trace of DQN agent\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the best policy with this agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNagent(env)\n",
    "agent.load_model(\"Q3a_best_model.mdl\")\n",
    "\n",
    "def run_simulation(agent,env,seed):\n",
    "    \"\"\" Run the simulation \"\"\"\n",
    "    log = []\n",
    "    finished = False\n",
    "    obs, info = env.reset(seed)\n",
    "    deaths = []\n",
    "    rewards = []\n",
    "    agent.reset()\n",
    "    agent.epsilon = 0\n",
    "    while not finished:\n",
    "        # Each action is done per week\n",
    "        action = agent.act(obs)\n",
    "        # Meaning also the log info is per week\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        deaths.append(info.total.dead)\n",
    "        rewards.append(R)\n",
    "        log.append(info) # save the information dict for logging\n",
    "\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    deaths = np.array(deaths)\n",
    "    rewards = np.array([r.numpy()[0] for r in rewards])\n",
    "    total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "    return deaths, rewards, total, cities, actions\n",
    "\n",
    "three_example_episodes = [run_simulation(agent,env,SEED+i) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the example episodes and plot the results\n",
    "deaths, rewards, total, cities, actions = three_example_episodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A plot of variables s e i r d per week total over time, where time is measured in weeks and all the variables share the y axis scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = set_size(400) if SAVE_REPORT else (8, 5)\n",
    "fig = plt.figure(figsize=size)\n",
    "[plt.plot(y) for y in total.values()]\n",
    "plt.legend([k.title() for k in total.keys()])\n",
    "plt.title('Full state information of the simulation')\n",
    "plt.ylabel('Number of people in each state')\n",
    "plt.xlabel('Week of episode')\n",
    "plt.tight_layout()\n",
    "if SAVE_REPORT:\n",
    "    plt.savefig('figures/created/Q1-fullState.png', dpi=300)\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A plot of variables i d total over time, where time is measured in weeks and all the variables share the y axis scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = set_size(400) if SAVE_REPORT else (8, 5)\n",
    "fig = plt.figure(figsize=size)\n",
    "[plt.plot(total[y]) for y in ['infected','dead']]\n",
    "plt.legend(['Infected','Dead'])\n",
    "plt.title('Observable state information of the simulation')\n",
    "plt.ylabel('Number of people in each state')\n",
    "plt.xlabel('Week of episode')\n",
    "plt.tight_layout()\n",
    "if SAVE_REPORT:\n",
    "    plt.savefig('figures/created/Q1-observableState.png', dpi=300)\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A set of plots of variables i d city over time, where time is measured in weeks (one subplot per-city, variables share the y-scaling per-city)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No modification necessary from the tutorial code\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "size = set_size(400) if SAVE_REPORT else (14, 10)\n",
    "fig = plt.figure(figsize=size)\n",
    "ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "[ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "[ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "[ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "[ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "ax_right['Z端rich'].set_xlabel('Week of episode')\n",
    "ax_right['Z端rich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "ax_right['Lausanne'].set_title('Observable state per city')\n",
    "plt.legend(['Infected','Dead'])\n",
    "fig.tight_layout()\n",
    "if SAVE_REPORT:\n",
    "    plt.savefig('figures/created/Q1-observableStatePerCity.png', dpi=300)\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try single plot\n",
    "size = set_size(700) if SAVE_REPORT else (14, 10)\n",
    "fig = plt.figure(figsize=size)\n",
    "ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "[ax_leftstate.plot(y) for y in total.values()]\n",
    "ax_leftstate.legend([t.title() for t in total.keys()])\n",
    "ax_leftstate.set_title('Full state information of the simulation')\n",
    "ax_leftstate.set_ylabel('Number of people in each state')\n",
    "\n",
    "[ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "ax_leftobs.legend(['Infected','Dead'])\n",
    "ax_leftobs.set_title('Observable state information of the simulation')\n",
    "ax_leftobs.set_ylabel('Number of people in each state')\n",
    "\n",
    "ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "ax_leftactions.set_title('Actions taken by the agent')\n",
    "ax_leftactions.set_yticks([0,1,2,3])\n",
    "ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "ax_leftactions.set_xlabel('Weeks in episode')\n",
    "\n",
    "[ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "[ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "[ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "[ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "ax_right['Z端rich'].set_xlabel('Weeks in episode')\n",
    "ax_right['Z端rich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "ax_right['Lausanne'].set_title('Observable state per city')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.tight_layout()\n",
    "if SAVE_REPORT:\n",
    "    plt.savefig('figures/created/Q1-complete.png', dpi=300)\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple simulations\n",
    "Running a suimulation of 50 enviroments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(agent,env,n):\n",
    "    info = []\n",
    "    for i in range(n):\n",
    "        # Run the agent with seed n, returns deaths, rewards, total, cities, actions\n",
    "        info.append(run_simulation(agent,env,i))\n",
    "    return info\n",
    "info = simulation(agent,env,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process simulations\n",
    "conf_days = [np.sum(c[4][\"confinement\"])*7 for c in info]\n",
    "rewards = [np.mean(c[1]) for c in info]\n",
    "deaths = [d[0][-1] for d in info] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" As it is deterministic we really don't need to get anything else than deaths and rewards \"\"\"\n",
    "\"\"\" Plot example \"\"\"\n",
    "fig, ax = plt.subplots(1,3,figsize=(12,4))\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n",
    "hist_avg(ax[0], deaths,'deaths')\n",
    "hist_avg(ax[1], rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], conf_days,'confined days')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print example \"\"\"\n",
    "print(f'Average death number: {np.mean(deaths)}')\n",
    "print(f'Average cumulative reward: {np.mean(rewards)}')\n",
    "print(f'Average number of confined days: {np.mean(conf_days)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ca8a34e536ae65e073071989487f43d99ac250feafd80421457c1fca6e7506f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
